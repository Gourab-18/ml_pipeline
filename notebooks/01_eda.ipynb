{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA) - Customer Churn Prediction\n",
        "\n",
        "## Overview\n",
        "This notebook performs comprehensive exploratory data analysis on the customer churn prediction dataset to understand data quality, feature distributions, and relationships with the target variable.\n",
        "\n",
        "## Objectives\n",
        "1. **Data Quality Assessment**: Missing values, outliers, data types\n",
        "2. **Feature Analysis**: Distributions, cardinality, correlations\n",
        "3. **Target Analysis**: Churn rate, class balance\n",
        "4. **Feature Engineering Decisions**: Preprocessing strategies for each feature\n",
        "5. **Data Leakage Check**: Identify and exclude leaky features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
        "\n",
        "from src.data.loader import DataLoader\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Seaborn version: {sns.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Initial Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data using our custom loader\n",
        "loader = DataLoader(\"configs/schema.yaml\")\n",
        "df = loader.load_data(\"data/sample.csv\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*50)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nLast 5 rows:\")\n",
        "display(df.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Quality Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Missing Percentage': missing_percent\n",
        "}).sort_values('Missing Percentage', ascending=False)\n",
        "\n",
        "print(\"Missing Values Summary:\")\n",
        "print(\"=\"*40)\n",
        "display(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "# Visualize missing values\n",
        "plt.figure(figsize=(12, 8))\n",
        "if missing_df['Missing Count'].sum() > 0:\n",
        "    # Create heatmap of missing values\n",
        "    plt.subplot(2, 1, 1)\n",
        "    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
        "    plt.title('Missing Values Heatmap')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Bar plot of missing percentages\n",
        "    plt.subplot(2, 1, 2)\n",
        "    missing_with_data = missing_df[missing_df['Missing Count'] > 0]\n",
        "    if len(missing_with_data) > 0:\n",
        "        sns.barplot(data=missing_with_data, x=missing_with_data.index, y='Missing Percentage')\n",
        "        plt.title('Missing Values by Column')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylabel('Missing Percentage (%)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No missing values found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data types analysis\n",
        "print(\"Data Types Summary:\")\n",
        "print(\"=\"*30)\n",
        "dtype_counts = df.dtypes.value_counts()\n",
        "print(dtype_counts)\n",
        "\n",
        "print(\"\\nDetailed Data Types:\")\n",
        "print(\"-\" * 30)\n",
        "for col in df.columns:\n",
        "    print(f\"{col:30} {str(df[col].dtype):15} {df[col].nunique():5} unique values\")\n",
        "\n",
        "# Check for potential data type issues\n",
        "print(\"\\nPotential Issues:\")\n",
        "print(\"-\" * 20)\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        # Check if numeric columns are stored as strings\n",
        "        try:\n",
        "            pd.to_numeric(df[col].dropna())\n",
        "            print(f\"⚠️  {col}: Numeric data stored as object\")\n",
        "        except:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Target Variable Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable analysis\n",
        "target_col = 'churn_probability'\n",
        "print(f\"Target Variable: {target_col}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Basic statistics\n",
        "print(\"Target Statistics:\")\n",
        "print(f\"Mean: {df[target_col].mean():.4f}\")\n",
        "print(f\"Median: {df[target_col].median():.4f}\")\n",
        "print(f\"Std: {df[target_col].std():.4f}\")\n",
        "print(f\"Min: {df[target_col].min():.4f}\")\n",
        "print(f\"Max: {df[target_col].max():.4f}\")\n",
        "\n",
        "# Class distribution\n",
        "print(f\"\\nClass Distribution:\")\n",
        "print(f\"Churn (1): {df[target_col].sum():.0f} ({df[target_col].mean()*100:.1f}%)\")\n",
        "print(f\"No Churn (0): {(1-df[target_col]).sum():.0f} ({(1-df[target_col].mean())*100:.1f}%)\")\n",
        "\n",
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(df[target_col], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Target Variable Distribution')\n",
        "axes[0].set_xlabel('Churn Probability')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(df[target_col].mean(), color='red', linestyle='--', label=f'Mean: {df[target_col].mean():.3f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Bar plot of class counts\n",
        "class_counts = df[target_col].value_counts().sort_index()\n",
        "axes[1].bar(class_counts.index, class_counts.values, color=['lightcoral', 'lightblue'])\n",
        "axes[1].set_title('Class Distribution')\n",
        "axes[1].set_xlabel('Churn (0=No, 1=Yes)')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_xticks([0, 1])\n",
        "\n",
        "# Add count labels on bars\n",
        "for i, v in enumerate(class_counts.values):\n",
        "    axes[1].text(i, v + 5, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features from target and leaky features\n",
        "leaky_features = loader.get_leaky_features()\n",
        "training_features = loader.get_training_features()\n",
        "\n",
        "print(\"Feature Categories:\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Total features: {len(df.columns)}\")\n",
        "print(f\"Training features (non-leaky): {len(training_features)}\")\n",
        "print(f\"Leaky features (exclude from training): {len(leaky_features)}\")\n",
        "print(f\"Target feature: 1\")\n",
        "\n",
        "print(f\"\\nLeaky features to exclude:\")\n",
        "for feature in leaky_features:\n",
        "    print(f\"  - {feature}\")\n",
        "\n",
        "print(f\"\\nTraining features:\")\n",
        "for feature in training_features:\n",
        "    print(f\"  - {feature}\")\n",
        "\n",
        "# Create feature analysis dataframe\n",
        "feature_analysis = []\n",
        "\n",
        "for col in df.columns:\n",
        "    if col != target_col:\n",
        "        feature_info = {\n",
        "            'feature': col,\n",
        "            'dtype': str(df[col].dtype),\n",
        "            'nunique': df[col].nunique(),\n",
        "            'missing_count': df[col].isnull().sum(),\n",
        "            'missing_pct': (df[col].isnull().sum() / len(df)) * 100,\n",
        "            'is_leaky': col in leaky_features,\n",
        "            'is_training': col in training_features\n",
        "        }\n",
        "        feature_analysis.append(feature_info)\n",
        "\n",
        "feature_df = pd.DataFrame(feature_analysis)\n",
        "feature_df = feature_df.sort_values(['is_leaky', 'nunique'], ascending=[True, False])\n",
        "\n",
        "print(\"\\nFeature Analysis Summary:\")\n",
        "print(\"=\"*50)\n",
        "display(feature_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cardinality analysis for categorical features\n",
        "print(\"Cardinality Analysis:\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "categorical_features = feature_df[feature_df['dtype'] == 'object']['feature'].tolist()\n",
        "high_cardinality_threshold = 50\n",
        "\n",
        "for feature in categorical_features:\n",
        "    cardinality = df[feature].nunique()\n",
        "    print(f\"{feature:30} {cardinality:5} unique values\")\n",
        "    \n",
        "    if cardinality > high_cardinality_threshold:\n",
        "        print(f\"  ⚠️  HIGH CARDINALITY - Consider embedding or target encoding\")\n",
        "    elif cardinality > 10:\n",
        "        print(f\"  ⚠️  MEDIUM CARDINALITY - Consider target encoding\")\n",
        "    else:\n",
        "        print(f\"  ✅ LOW CARDINALITY - Safe for one-hot encoding\")\n",
        "\n",
        "# Numeric features analysis\n",
        "print(f\"\\nNumeric Features Analysis:\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "numeric_features = feature_df[\n",
        "    (feature_df['dtype'].isin(['int64', 'float64'])) & \n",
        "    (feature_df['is_training'] == True)\n",
        "]['feature'].tolist()\n",
        "\n",
        "for feature in numeric_features:\n",
        "    stats = df[feature].describe()\n",
        "    print(f\"{feature:30}\")\n",
        "    print(f\"  Range: {stats['min']:.2f} to {stats['max']:.2f}\")\n",
        "    print(f\"  Mean: {stats['mean']:.2f}, Std: {stats['std']:.2f}\")\n",
        "    \n",
        "    # Check for outliers using IQR method\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df[(df[feature] < Q1 - 1.5*IQR) | (df[feature] > Q3 + 1.5*IQR)][feature]\n",
        "    print(f\"  Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions for key numeric features\n",
        "key_numeric_features = ['age', 'monthly_revenue', 'login_frequency', 'session_duration_avg', 'support_tickets_count']\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(key_numeric_features):\n",
        "    if i < len(axes) and feature in df.columns:\n",
        "        axes[i].hist(df[feature].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[i].set_title(f'{feature} Distribution')\n",
        "        axes[i].set_xlabel(feature)\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        \n",
        "        # Add statistics\n",
        "        mean_val = df[feature].mean()\n",
        "        median_val = df[feature].median()\n",
        "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.1f}')\n",
        "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.1f}')\n",
        "        axes[i].legend()\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(len(key_numeric_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features distributions\n",
        "categorical_features = ['gender', 'location_country', 'subscription_type']\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    if feature in df.columns:\n",
        "        value_counts = df[feature].value_counts()\n",
        "        \n",
        "        axes[i].bar(range(len(value_counts)), value_counts.values, color='lightcoral')\n",
        "        axes[i].set_title(f'{feature} Distribution')\n",
        "        axes[i].set_xlabel(feature)\n",
        "        axes[i].set_ylabel('Count')\n",
        "        axes[i].set_xticks(range(len(value_counts)))\n",
        "        axes[i].set_xticklabels(value_counts.index, rotation=45)\n",
        "        \n",
        "        # Add count labels on bars\n",
        "        for j, v in enumerate(value_counts.values):\n",
        "            axes[i].text(j, v + 1, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Target vs Feature Relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis for numeric features\n",
        "numeric_cols = [col for col in training_features if df[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    # Calculate correlation with target\n",
        "    correlations = df[numeric_cols + [target_col]].corr()[target_col].drop(target_col).sort_values(key=abs, reverse=True)\n",
        "    \n",
        "    print(\"Feature Correlations with Target:\")\n",
        "    print(\"=\"*40)\n",
        "    for feature, corr in correlations.items():\n",
        "        print(f\"{feature:30} {corr:6.3f\")\n",
        "    \n",
        "    # Visualize correlations\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    correlations.plot(kind='barh', color='skyblue')\n",
        "    plt.title('Feature Correlations with Target Variable')\n",
        "    plt.xlabel('Correlation Coefficient')\n",
        "    plt.ylabel('Features')\n",
        "    plt.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Correlation heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = df[numeric_cols + [target_col]].corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                square=True, fmt='.2f')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric features found for correlation analysis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target vs categorical features\n",
        "categorical_cols = [col for col in training_features if df[col].dtype == 'object']\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    fig, axes = plt.subplots(1, len(categorical_cols), figsize=(5*len(categorical_cols), 5))\n",
        "    if len(categorical_cols) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, feature in enumerate(categorical_cols):\n",
        "        # Calculate churn rate by category\n",
        "        churn_by_category = df.groupby(feature)[target_col].agg(['count', 'sum', 'mean']).reset_index()\n",
        "        churn_by_category.columns = [feature, 'total_count', 'churn_count', 'churn_rate']\n",
        "        churn_by_category = churn_by_category.sort_values('churn_rate', ascending=False)\n",
        "        \n",
        "        # Plot\n",
        "        bars = axes[i].bar(range(len(churn_by_category)), churn_by_category['churn_rate'], \n",
        "                          color='lightcoral', alpha=0.7)\n",
        "        axes[i].set_title(f'Churn Rate by {feature}')\n",
        "        axes[i].set_xlabel(feature)\n",
        "        axes[i].set_ylabel('Churn Rate')\n",
        "        axes[i].set_xticks(range(len(churn_by_category)))\n",
        "        axes[i].set_xticklabels(churn_by_category[feature], rotation=45)\n",
        "        \n",
        "        # Add count labels\n",
        "        for j, (rate, count) in enumerate(zip(churn_by_category['churn_rate'], churn_by_category['total_count'])):\n",
        "            axes[i].text(j, rate + 0.01, f'{count}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed statistics\n",
        "    print(\"Categorical Feature Analysis:\")\n",
        "    print(\"=\"*40)\n",
        "    for feature in categorical_cols:\n",
        "        print(f\"\\n{feature}:\")\n",
        "        churn_by_category = df.groupby(feature)[target_col].agg(['count', 'sum', 'mean']).reset_index()\n",
        "        churn_by_category.columns = [feature, 'total_count', 'churn_count', 'churn_rate']\n",
        "        churn_by_category = churn_by_category.sort_values('churn_rate', ascending=False)\n",
        "        display(churn_by_category)\n",
        "else:\n",
        "    print(\"No categorical features found for analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Engineering Decisions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create feature engineering decisions\n",
        "feature_decisions = []\n",
        "\n",
        "for feature in df.columns:\n",
        "    if feature == target_col:\n",
        "        continue\n",
        "        \n",
        "    # Determine feature type and preprocessing strategy\n",
        "    dtype = str(df[feature].dtype)\n",
        "    cardinality = df[feature].nunique()\n",
        "    missing_pct = (df[feature].isnull().sum() / len(df)) * 100\n",
        "    is_leaky = feature in leaky_features\n",
        "    \n",
        "    # Determine preprocessing action\n",
        "    if is_leaky:\n",
        "        action = \"drop\"\n",
        "        reason = \"Leaky feature - only available post-label\"\n",
        "    elif dtype == 'object':\n",
        "        if cardinality <= 10:\n",
        "            action = \"onehot\"\n",
        "            reason = f\"Low cardinality categorical ({cardinality} unique values)\"\n",
        "        elif cardinality <= 50:\n",
        "            action = \"target_encode\"\n",
        "            reason = f\"Medium cardinality categorical ({cardinality} unique values)\"\n",
        "        else:\n",
        "            action = \"embed\"\n",
        "            reason = f\"High cardinality categorical ({cardinality} unique values)\"\n",
        "    elif dtype in ['int64', 'float64']:\n",
        "        # Check if it's actually categorical (low unique values)\n",
        "        if cardinality <= 20 and df[feature].dtype == 'int64':\n",
        "            action = \"onehot\"\n",
        "            reason = f\"Integer with low cardinality ({cardinality} unique values)\"\n",
        "        else:\n",
        "            action = \"scale\"\n",
        "            reason = f\"Numeric feature - requires scaling\"\n",
        "    else:\n",
        "        action = \"investigate\"\n",
        "        reason = f\"Unknown data type: {dtype}\"\n",
        "    \n",
        "    feature_decisions.append({\n",
        "        'name': feature,\n",
        "        'type': dtype,\n",
        "        'cardinality': cardinality,\n",
        "        'missing_pct': missing_pct,\n",
        "        'is_leaky': is_leaky,\n",
        "        'action': action,\n",
        "        'reason': reason\n",
        "    })\n",
        "\n",
        "# Create DataFrame and sort by action\n",
        "feature_decisions_df = pd.DataFrame(feature_decisions)\n",
        "feature_decisions_df = feature_decisions_df.sort_values(['action', 'cardinality'])\n",
        "\n",
        "print(\"Feature Engineering Decisions:\")\n",
        "print(\"=\"*60)\n",
        "display(feature_decisions_df)\n",
        "\n",
        "# Summary by action\n",
        "print(\"\\nSummary by Preprocessing Action:\")\n",
        "print(\"=\"*40)\n",
        "action_summary = feature_decisions_df['action'].value_counts()\n",
        "for action, count in action_summary.items():\n",
        "    print(f\"{action:15} {count:3} features\")\n",
        "    \n",
        "    # Show examples\n",
        "    examples = feature_decisions_df[feature_decisions_df['action'] == action]['name'].tolist()[:3]\n",
        "    if examples:\n",
        "        print(f\"  Examples: {', '.join(examples)}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export feature list to CSV\n",
        "feature_list_path = \"configs/feature_list.csv\"\n",
        "feature_decisions_df[['name', 'type', 'cardinality', 'action']].to_csv(feature_list_path, index=False)\n",
        "print(f\"Feature list exported to: {feature_list_path}\")\n",
        "\n",
        "# Create EDA summary\n",
        "eda_summary = f\"\"\"\n",
        "# EDA Summary - Customer Churn Prediction\n",
        "\n",
        "## Dataset Overview\n",
        "- **Total Records**: {len(df):,}\n",
        "- **Total Features**: {len(df.columns)}\n",
        "- **Training Features**: {len(training_features)}\n",
        "- **Leaky Features**: {len(leaky_features)}\n",
        "- **Target Variable**: {target_col}\n",
        "\n",
        "## Data Quality\n",
        "- **Missing Values**: {df.isnull().sum().sum()} total missing values\n",
        "- **Features with Missing Data**: {(df.isnull().sum() > 0).sum()}\n",
        "- **Memory Usage**: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n",
        "\n",
        "## Target Variable\n",
        "- **Churn Rate**: {df[target_col].mean():.1%}\n",
        "- **Class Balance**: {'Balanced' if 0.3 <= df[target_col].mean() <= 0.7 else 'Imbalanced'}\n",
        "\n",
        "## Feature Engineering Summary\n",
        "\"\"\"\n",
        "\n",
        "for action in feature_decisions_df['action'].unique():\n",
        "    count = (feature_decisions_df['action'] == action).sum()\n",
        "    eda_summary += f\"- **{action.title()}**: {count} features\\n\"\n",
        "\n",
        "eda_summary += f\"\"\"\n",
        "## Key Findings\n",
        "1. **Data Leakage**: {len(leaky_features)} features identified as leaky and excluded from training\n",
        "2. **Missing Data**: {(df.isnull().sum() > 0).sum()} features have missing values requiring imputation\n",
        "3. **Feature Types**: {len([f for f in training_features if df[f].dtype == 'object'])} categorical, {len([f for f in training_features if df[f].dtype in ['int64', 'float64']])} numeric\n",
        "4. **High Cardinality**: {len(feature_decisions_df[feature_decisions_df['cardinality'] > 50])} features with >50 unique values\n",
        "\n",
        "## Recommendations\n",
        "1. **Preprocessing**: Implement feature-specific preprocessing based on feature_list.csv\n",
        "2. **Missing Values**: Use appropriate imputation strategies for each feature type\n",
        "3. **Feature Selection**: Consider feature importance analysis after preprocessing\n",
        "4. **Model Validation**: Use stratified sampling due to class imbalance\n",
        "\"\"\"\n",
        "\n",
        "# Save EDA summary\n",
        "with open(\"docs/eda_summary.md\", \"w\") as f:\n",
        "    f.write(eda_summary)\n",
        "\n",
        "print(\"EDA summary exported to: docs/eda_summary.md\")\n",
        "print(\"\\nEDA Analysis Complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
